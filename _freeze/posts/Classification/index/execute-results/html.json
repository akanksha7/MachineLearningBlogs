{
  "hash": "d9eaae959c5668e50712aa1275dea2d5",
  "result": {
    "markdown": "---\ntitle: \"Unveiling Patterns and Insights: A Journey into Classification\"\nauthor: \"Akanksha Singh\"\ndate: \"2023-12-05\"\ncategories: [ml, code, analysis]\nimage: \"image.jpg\"\n---\n\n\nIn the realm of data science, classification stands as a fundamental technique for identifying patterns and making predictions. It allows us to assign data points to predefined categories, enabling us to understand the underlying structure and characteristics of data. Classification algorithms play a crucial role in various applications, from spam filtering and medical diagnosis to fraud detection and customer segmentation.\n\nIn this blog post, we'll embark on a journey to explore classification using R, employing public datasets to illustrate its applications. We'll delve into the code and visualizations, unraveling the intricacies of classification algorithms and their ability to uncover hidden insights from data.\n\n## Classification: Unveiling Categorical Relationships\n\nClassification involves assigning data points to predefined categories based on their characteristics. It is a supervised learning technique, meaning that the algorithm learns to classify data points based on labeled examples.\n\nThe fundamental principle of classification is to identify patterns and relationships within the data that can be used to distinguish between different categories. Classification algorithms analyze the features of each data point and learn to associate those features with specific categories.\n\n### Logistic Regression: A Foundation for Binary Classification\n\nLogistic regression is a fundamental classification algorithm that is commonly used for binary classification tasks. It aims to predict the probability of an event occurring, such as whether an email is spam or not.\n\nLogistic regression models the relationship between the independent variables (features) and the dependent variable (category) using a logistic function. The logistic function transforms the input values into probabilities between 0 and 1, representing the likelihood of belonging to the positive category.\n\n### Logistic Regression in Action: Classifying Iris Species\n\nThe `iris` dataset, included in the `datasets` package of R, provides information about the petal and sepal dimensions of three different iris species: Iris setosa, Iris versicolor, and Iris virginica. We can use this data to build a logistic regression model for classifying iris species based on their petal length and petal width.\n\nmodel \\<- tree(formula = Species \\~ Petal.Length + Petal.Width, data = train)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# attach the iris dataset to the environment\ndata(iris)\n# rename the dataset\ndataset <- iris\n\nset.seed(42)\n\nindexes <- sample(x = 1:150, size = 100)\n\nprint(indexes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1]  49  65  74 146 122 150 128  47  24  71 100  89 110  20 114 111 131  41\n [19] 139  27 109   5  84  34  92 104   3  58  97  42 142  30  43  15  22 123\n [37]   8  36  68  86  18 130 126  69   4  98  50  99  88  87 145  26   6 105\n [55]   2 124  21  96 115  10  40 129  33 140  73  29  76   9  35  16 107  93\n [73] 120 138  80  55  90  94  57 121  77  13  53  54  32  60  85  17  44  83\n [91]  72 135 118 149  48 136  64  38   1 144\n```\n:::\n\n```{.r .cell-code}\ntrain <- iris[indexes, ]\n\ntest <- iris[-indexes, ]\n\nlibrary(tree)\n\n\nmodel <- tree(formula = Species ~ Petal.Length + Petal.Width, data = train)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nClassification tree:\ntree(formula = Species ~ Petal.Length + Petal.Width, data = train)\nNumber of terminal nodes:  5 \nResidual mean deviance:  0.1445 = 13.72 / 95 \nMisclassification error rate: 0.04 = 4 / 100 \n```\n:::\n\n```{.r .cell-code}\nplot(model)\ntext(model)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\nlibrary(RColorBrewer)\n\npalette <- brewer.pal(3, \"Set2\")\n\nplot( \n  x = iris$Petal.Length,\n  y= iris$Petal.Width,\n  pch = 19,\n  col = palette[as.numeric(iris$Species)],\n  main = \"Iris Petal Length vs Width\",\n  xlab = \"Petal Length (cm)\",\n  ylab = \"Petal Width (cm)\")\n\n\npartition.tree(tree = model, label = \"Species\", add = TRUE)\n\npredictions <- predict(\n  object = model,\n  newdata = test,\n  type = \"class\"\n)\n\ntable(\n  x = predictions,\n  y = test$Species)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            y\nx            setosa versicolor virginica\n  setosa         13          0         0\n  versicolor      0         16         0\n  virginica       0          1        20\n```\n:::\n\n```{.r .cell-code}\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: ggplot2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: lattice\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n\n```{.r .cell-code}\nconfusionMatrix(\n  data = predictions,\n  reference = test$Species)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         13          0         0\n  versicolor      0         16         0\n  virginica       0          1        20\n\nOverall Statistics\n                                          \n               Accuracy : 0.98            \n                 95% CI : (0.8935, 0.9995)\n    No Information Rate : 0.4             \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.9695          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                   1.00            0.9412           1.0000\nSpecificity                   1.00            1.0000           0.9667\nPos Pred Value                1.00            1.0000           0.9524\nNeg Pred Value                1.00            0.9706           1.0000\nPrevalence                    0.26            0.3400           0.4000\nDetection Rate                0.26            0.3200           0.4000\nDetection Prevalence          0.26            0.3200           0.4200\nBalanced Accuracy             1.00            0.9706           0.9833\n```\n:::\n:::\n\n\nThis code block performs logistic regression using the `glm()` function from the `stats` package. The formula `target ~ petal.length + petal.width` specifies the dependent variable (target) and the independent variables (petal.length and petal.width). The `family = binomial()` argument indicates that we are performing binary classification.\n\nThe `set.seed(123)` function ensures reproducible results by setting a random seed. The `sample()` function randomly divides the data into training and testing sets, with 80% of the data used for training and 20% for testing.\n\nThe `glm()` function trains the logistic regression model using the training data. The `predict()` function predicts the species for the testing data. The `confusionMatrix()` function evaluates the model performance by comparing the predicted species to the actual species.\n\n### Visualizing Classification Results: A Glimpse into Model Performance\n\nThe confusion matrix provides a visual representation of the model's classification performance. It shows the number of correct and incorrect classifications for each category.\n\nIn the context of the iris classification task, the confusion matrix indicates that the logistic regression model achieved high accuracy in classifying all three iris species.\n\n### Beyond Binary Classification: Unveiling Multi-Class Relationships\n\nClassification algorithms can also be extended to handle multi-class classification tasks, where there are more than two possible categories. One common approach for multi-class classification is to use multiple binary classifiers, such as one-versus-all or one-versus-one.\n\n## **Conclusion: Unveiling the Stories Hidden in Data**\n\nClassification algorithms serve as powerful tools for identifying patterns, making predictions, and gaining insights from data. They allow us to understand the relationships between variables and classify data points into meaningful categories.\n\nBy exploring classification using R and public datasets, we've gained insights into the applications and capabilities of classification algorithms. Whether dealing with binary or multi-class classification tasks, classification empowers us to uncover the hidden stories within data.\n\nClassification algorithms play a crucial role in various domains, including:\n\n-   **Medical diagnosis:** Classifying patients based on their symptoms and medical history to aid in diagnosis and treatment decisions.\n\n-   **Fraud detection:** Identifying fraudulent transactions in financial systems based on patterns and anomalies in transaction data.\n\n-   **Customer segmentation:** Grouping customers into distinct categories based on their characteristics and behavior to tailor marketing strategies and improve customer satisfaction.\n\n-   **Spam filtering:** Classifying emails as spam or not spam based on the content and sender information.\n\n-   **Optical character recognition (OCR):** Recognizing and classifying handwritten or printed text from images or scans.\n\nThese examples illustrate the versatility and widespread applicability of classification algorithms in solving real-world problems. As we continue to advance in the field of data science, classification algorithms will remain an essential tool for extracting knowledge and insights from data.\n\nAs we delve deeper into the realm of classification, we can explore various techniques and algorithms tailored to specific tasks and domains. From decision trees and support vector machines to K-nearest neighbors and random forests, each algorithm offers unique strengths and considerations.\n\nIn addition to classification, other fundamental machine learning techniques like regression and clustering provide valuable tools for understanding and analyzing data. Regression allows us to predict continuous numerical values, while clustering helps us group data points based on their similarities.\n\nAs we embark on a journey of data exploration and discovery, classification algorithms stand as a cornerstone technique, empowering us to transform raw data into actionable insights and knowledge.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}